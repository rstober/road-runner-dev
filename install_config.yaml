---

install_dir: /root/.road-runner
tmp_dir: /root/.road-runner/tmp
tmp_dir2: /root/.road-runner/tmp2
pb_dir: /root/.road-runner/pb
update_head_node: no
ansible_version: 2.10.*
clone_software_image: yes

apps:
  disabled: yes

software_images:
  - name: default-image-orig
    clone_from: default-image
    path: /cm/images/default-image-orig
  - name: dgx-a100-image-orig
    clone_from: dgx-a100-image
    path: /cm/images/dgx-a100-image-orig
  - name: k8s-image
    clone_from: default-image
    path: /cm/images/k8s-image
        
categories:
  - name: dgx
    clone_from: default
    software_image: default-image
  - name: jup
    clone_from: default
    software_image: default-image
    
nodes:
  - hostname: node01
    category: jup
    power_control: custom
    custom_power_script: /cm/local/apps/cmd/scripts/powerscripts/os-power-control.py
  - hostname: node02
    category: cloned
    power_control: custom
    custom_power_script: /cm/local/apps/cmd/scripts/powerscripts/os-power-control.py
  - hostname: node03
    category: cloned
    power_control: custom
    custom_power_script: /cm/local/apps/cmd/scripts/powerscripts/os-power-control.py
  - hostname: node04
    category: cloned
    power_control: custom
    custom_power_script: /cm/local/apps/cmd/scripts/powerscripts/os-power-control.py

packages:
  - package_name: cuda11.7-toolkit
    target: headnode
  - package_name: cuda-driver
    target: /cm/images/cloned-image
  - package_name: cuda-dcgm
    target: /cm/images/cloned-image
  - package_name: '@workstation'
    target: /cm/images/cloned-image
    
csps:
  - name: amazon
    type: aws
    useMarketplaceAMIs: AS_NEEDED
    
users:
  - rstober
  - yangya
    
wlms:
  - name: slurm
    constrain_devices: yes
    queues:
      - queue_name: gpu
        clone_from: defq
        default_queue: false
        over_subscribe: yes:4
        wlm_cluster: slurm
      - queue_name: jup
        clone_from: defq
        default_queue: false
        over_subscribe: yes:4
        wlm_cluster: slurm
    configuration_overlays:
      - name: slurm-client
        categories: ['cloned']
        allHeadNodes: false
        roles:
          - name: slurmclient
            wlm_cluster: slurm
            queues: ['gpu']
            sockets_per_board: 1
            cores_per_socket: 2
            threads_per_core: 2
            slots: 4
            real_memory: 15535
            generic_resources:
              - name: gpu
                alias: gpu0
                file: /dev/nvidia0
                type: t4
                count: 1
                consumable: true
                add_to_gres_config: true
      - name: slurm-jupyter
        categories: ['jup']
        allHeadNodes: false
        roles:
          - name: slurmclient
            wlm_cluster: slurm
            queues: ['jup']
            sockets_per_board: 1
            cores_per_socket: 2
            threads_per_core: 2
            slots: 4
            real_memory: 15535
            generic_resources:
              - name: gpu
                alias: gpu0
                file: /dev/nvidia0
                type: t4
                count: 1
                consumable: true
                add_to_gres_config: true

autoscaler:
    name: auto-scaler
    categories: []
    allHeadNodes: true
    roles:
      - name: scaleserver
        runInterval: 60
        debug: true
        index: 0        
        resource_providers:
          - provider_name: aws
            type: ScaleDynamicNodesProvider
            templateNode: node02
            startTemplateNode: false
            stopTemplateNode: false
            nodeRange: node02..node04
            networkInterface: bootif
            defaultResources: "['cpus=4','mem_free:slurm=16GB','gpu_free:t4:slurm=1']"
        engines:
          - name: slurm
            type: ScaleHpcEngine
            workloadsPerNode: 4
            priority: 10
            wlmCluster: slurm
            trackers:
              - name: gpu
                type: ScaleHpcQueueTracker
                queue: gpu
                assignCategory: cloned
                allowedResourceProviders: ['aws']
                workloadsPerNode: 4
 
jupyter:
    server: jupyter
    wlm_queues: ['jup']
    wlm_categories: jup
    wlm_nodes:

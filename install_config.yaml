---

install_dir: /root/.road-runner
tmp_dir: /root/.road-runner/tmp
pb_dir: /root/.road-runner/pb
update_head_node: no
ansible_version: 2.10.*
clone_software_image: yes
template_node: node01
template_ip: 10.141.255.250
template_name: template
template_nic: bootif

apps:
  disabled: yes
  
networks:
  - name: ibnet
    clone_from: internalnet
    domainname: ibnet.cluster.local
    baseaddress: 10.149.0.0
    broadcastaddress: 10.149.255.255
    netmaskbits: 16
    mtu: 9000
    management: no
  - name: storagenet
    clone_from: internalnet
    domainname: storagenet.cluster.local
    baseaddress: 10.130.123.0
    broadcastaddress: 10.130.123.255
    netmaskbits: 24
    mtu: 1500
    management: no

software_images:
  - name: default-image
    backup: default-image-orig
    clone_from: default-image
    path: /cm/images/default-image-orig
    kernel_release: 5.15.0-50-generic
    modules:
    packages:
    create_root_dirs:
  - name: dgx-os-5.4-a100-image
    backup: dgx-os-5.4-a100-image-orig
    clone_from: dgx-os-5.4-a100-image
    path: /cm/images/dgx-os-5.4-a100-image-orig
    kernel_release: 5.4.0-124-generic
    modules: 
      - name: bonding
        parameters: ''
      - name: raid0
        parameters: ''
      - name: raid1
        parameters: ''
      - name: mlx5_core
        parameters: ''
    packages: ['ifenslave']
    create_root_dirs: ['/raid']
  - name: k8s-master-image
    backup: k8s-master-image
    clone_from: default-image
    path: /cm/images/k8s-master-image
    kernel_release: 5.15.0-50-generic
    modules:
      - name: bonding
        parameters: ''
      - name: mlx5_core
        parameters: ''
    packages: ['ifenslave']
    create_root_dirs:
        
categories:
  - name: dgx
    clone_from: default
    software_image: dgx-a100-image
    disksetup: dgxa100-disksetup.xml
  - name: k8s
    clone_from: default
    software_image: k8s-image
    disksetup: k8s-one-big-partition.xml
    
nodes:
  - hostname: dgx01
    category: dgx
    clone_from: node01
    nics:
      - device: BOOTIF
        ip: 10.141.0.1
        network: internalnet
  - hostname: dgx02
    category: dgx
    clone_from: node01
    nics:
      - device: BOOTIF
        ip: 10.141.0.2
        network: internalnet
  - hostname: dgx03
    category: dgx
    clone_from: node01
    nics:
      - device: BOOTIF
        ip: 10.141.0.3
        network: internalnet
  - hostname: dgx04
    category: dgx
    clone_from: node01
    nics:
      - device: BOOTIF
        ip: 10.141.0.4
        network: internalnet
  - hostname: knode01
    category: k8s
    clone_from: node01
    nics:
      - device: BOOTIF
        ip: 10.141.0.5
        network: internalnet
  - hostname: knode02
    category: k8s
    clone_from: node01
    nics:
      - device: BOOTIF
        ip: 10.141.0.6
        network: internalnet
  - hostname: knode03
    category: k8s
    clone_from: node01
    nics:
      - device: BOOTIF
        ip: 10.141.0.7
        network: internalnet
    
csps:
  - name: amazon
    type: aws
    useMarketplaceAMIs: AS_NEEDED
    
users:
  - rstober
  - yangya
    
wlms:
  - name: slurm
    constrain_devices: yes
    queues:
      - queue_name: gpu
        clone_from: defq
        default_queue: false
        over_subscribe: yes:4
        wlm_cluster: slurm
      - queue_name: jup
        clone_from: defq
        default_queue: false
        over_subscribe: yes:4
        wlm_cluster: slurm
    configuration_overlays:
      - name: slurm-client
        categories: ['cloned']
        allHeadNodes: false
        roles:
          - name: slurmclient
            wlm_cluster: slurm
            queues: ['gpu']
            sockets_per_board: 1
            cores_per_socket: 2
            threads_per_core: 2
            slots: 4
            real_memory: 15535
            generic_resources:
              - name: gpu
                alias: gpu0
                file: /dev/nvidia0
                type: t4
                count: 1
                consumable: true
                add_to_gres_config: true
      - name: slurm-jupyter
        categories: ['jup']
        allHeadNodes: false
        roles:
          - name: slurmclient
            wlm_cluster: slurm
            queues: ['jup']
            sockets_per_board: 1
            cores_per_socket: 2
            threads_per_core: 2
            slots: 4
            real_memory: 15535
            generic_resources:
              - name: gpu
                alias: gpu0
                file: /dev/nvidia0
                type: t4
                count: 1
                consumable: true
                add_to_gres_config: true

autoscaler:
    name: auto-scaler
    categories: []
    allHeadNodes: true
    roles:
      - name: scaleserver
        runInterval: 60
        debug: true
        index: 0        
        resource_providers:
          - provider_name: aws
            type: ScaleDynamicNodesProvider
            templateNode: node02
            startTemplateNode: false
            stopTemplateNode: false
            nodeRange: node02..node04
            networkInterface: BOOTIF
            defaultResources: "['cpus=4','mem_free:slurm=16GB','gpu_free:t4:slurm=1']"
        engines:
          - name: slurm
            type: ScaleHpcEngine
            workloadsPerNode: 4
            priority: 10
            wlmCluster: slurm
            trackers:
              - name: gpu
                type: ScaleHpcQueueTracker
                queue: gpu
                assignCategory: cloned
                allowedResourceProviders: ['aws']
                workloadsPerNode: 4
 
jupyter:
    server: jupyter
    wlm_queues: ['jup']
    wlm_categories: jup
    wlm_nodes:
